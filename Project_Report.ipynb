{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fed54f1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "suppressPackageStartupMessages(\n",
    "    {suppressWarnings({\n",
    "        library(tidyverse)\n",
    "        library(repr)\n",
    "        library(tidymodels)\n",
    "        library(tidyr)\n",
    "        library(ggplot2)        \n",
    "        library(scales)\n",
    "        library(patchwork) \n",
    "        library(purrr)\n",
    "        library(dplyr)\n",
    "        library(GGally)\n",
    "        library(ISLR)\n",
    "    })\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00f46907",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Size and general style set up\n",
    "options(repr.plot.width = 6, repr.plot.height = 4, repr.matrix.max.rows = 7,readr.show_col_types = FALSE)\n",
    "\n",
    "# Load Data\n",
    "player_data <- read_csv(\"https://raw.githubusercontent.com/FabianoGLentini/player-subscription-ml/refs/heads/main/data/players.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5a5de9",
   "metadata": {},
   "source": [
    "# Data Science Project: Project Final Report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b72ca2",
   "metadata": {},
   "source": [
    "## Introduction: \n",
    "For this project, we are analyzing data collected by researchers in Computer Science at UBC. They have set up a MineCraft server that allows them to record consenting players’ behaviour and characteristics for study. In particular, we want to answer the question: **What players' `Age`, `gender`, and `played_hours` can best predict whether they would `subscribe` to a gaming newsletter, and how does it differ from players `experience`?** In this project, we will focus on the `players.csv` dataset, as it is of most use to us to answer our question. The `players.csv` set contains 196 observations and 7 variables. This data is already in its tidy form because every column is a single variable, every row is a single observation, and every cell is a single value.  We can see that there are three types of variables here: character, logical, and double. \n",
    "\n",
    "**Character Variables:**\n",
    "\n",
    "`hashedEmail` (email of player that has been converted into a unique string of characters, for privacy)\n",
    "\n",
    "`name` (first name of player)\n",
    "\n",
    "`gender` (gender of player)\n",
    "\n",
    "`experience` (skill level of player)\n",
    "\n",
    "**Double Variables:**\n",
    "\n",
    "`played_hours` (number of hours spent on the game by each player)\n",
    "\n",
    "`Age` (age in years of each player)\n",
    "\n",
    "**Logical variable:**\n",
    "\n",
    "`subscribe` (whether or not the player is subscribed to the game newsletter) \n",
    "\n",
    "For our question, we want to know if a player's age, gender, and amount of played hours can determine if they will subscribe to the game newsletter or not. We chose these three predictor variables because they are all characteristics of each individual player that can allow us to group them and determine which type of player is most likely to subscribe. Additionally, we want to determine if any difference in experience across players will be a factor in whether or not a player is subscribed.\n",
    "\n",
    "A potential issue with this data under the `played_hours` variable, we can see that many players have 0 hours played. This could cause weird results when we begin to wrangle. Furthermore, some values under the `gender` variable contain very little sample sizes, making it hard to scale these categories. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f450c1e",
   "metadata": {},
   "source": [
    "## Methods & Results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1ee4a7",
   "metadata": {},
   "source": [
    "### Set up and intro to data: \"TODO should rename later.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "716f4755",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“\u001b[1m\u001b[22mThere was 1 warning in `mutate()`.\n",
      "\u001b[1m\u001b[22m\u001b[36mℹ\u001b[39m In argument: `gender = fct_recode(...)`.\n",
      "Caused by warning:\n",
      "\u001b[33m!\u001b[39m Unknown levels in `f`: Other”\n"
     ]
    }
   ],
   "source": [
    "# Wrangle data\n",
    "#TODO will need additional wranggling due to poor or few varied sample categories\n",
    "player_df <- player_data |> # TODO must consider reworking a joining of some labels in gender etc due to low representaions\n",
    "            select( subscribe, gender, played_hours, experience, Age) |>\n",
    "            drop_na() |>\n",
    "            mutate(\n",
    "                subscribe = as_factor(subscribe),  # lgl -> fct for analysis and modeling\n",
    "                gender = as_factor(gender),        # chr -> fct for analysis and modeling\n",
    "                played = factor(as.logical(played_hours)), # TODO may not use \n",
    "                experience = as_factor(experience) # chr -> fct for analysis and modeling\n",
    "            ) |>\n",
    "             mutate(gender = fct_recode(gender, # Aggregate some of the gender to balance out count and avoid missclassification due to underepresented categories\n",
    "                                    \"Non-binary/Other\" = \"Agender\",\n",
    "                                    \"Non-binary/Other\" = \"Non-binary\",\n",
    "                                     \"Non-binary/Other\" = \"Two-Spirited\",\n",
    "                                    \"Non-binary/Other\" = \"Other\"\n",
    "                                      ))\n",
    " # Removed row with NA values, as it may distort the model      \n",
    "\n",
    "# TODO FABIO check if player_hours should adjust to use a binary outcome, \n",
    "# either played or didn't play at all, or if any of the predictor should be removed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d42c93",
   "metadata": {},
   "source": [
    "### Training and Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4749e0c9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# head(player_df) # TODO DELETE tmp  for set up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62fe84df",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# SPLIT/SCALE DATA\n",
    "set.seed(2025) # Don't change\n",
    "# Prep for modelling\n",
    "ply_df <- select(player_df, Age, subscribe, gender, played_hours, experience)  # Exclude row_id & experience for modeling purposes\n",
    "\n",
    "# Split step\n",
    "player_split <- initial_split(ply_df, prop = 0.70, strata = subscribe) \n",
    "player_train <- training(player_split)\n",
    "player_test <- testing(player_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cd696bb-6863-4879-94fe-acd57b748007",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Training/Testing/Total>\n",
       "<135/59/194>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#TODO DELETE \n",
    "## tr_sub_counts <- ggplot(player_train, aes(x = subscribe, fill = subscribe)) +\n",
    "#                 geom_bar() + \n",
    "#                 ylim(c(0,100)) + \n",
    "#                 guides(fill = \"none\") +\n",
    "#                 labs(x = \"Subscribe status\", y = \"Count of players\") + \n",
    "#                 ggtitle(\"Training set\")\n",
    "                \n",
    "# tst_sub_counts <- tr_sub_counts %+% player_test +\n",
    "#                 ggtitle(\"Testing Set\")\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "# tr_sub_counts + tst_sub_counts\n",
    "\n",
    "player_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8405a571-c589-46f7-9011-4a8a5b1dde88",
   "metadata": {},
   "source": [
    "**Data Split:**\n",
    "\n",
    "Due to a small, fairly imbalanced data set of 194 data points, we will use an 'initial_split' of 70% (135) training and 30% (59) testing sets to try to balance the split across all our predictors. Our goal is to train a model to predict the outcome of subscription, thus our strata will be set to `subscribe` variable. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96f5e2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recipes:\n",
    "# Scale/Recipe\n",
    "\n",
    "# Note: A = Age, G = gender, H = played_hours and E = experience\n",
    "# Recipe 01:\n",
    "# Age + gender + played_hours\n",
    "rc_AGH <- recipe(subscribe ~ Age + gender + played_hours, data = player_train) |>\n",
    "            step_dummy(all_nominal_predictors()) |>\n",
    "            step_log(played_hours, Age) |>\n",
    "            step_zv(all_predictors()) |> # Used to remove zero-variance variable after wrangling, gender_Other category had no rows associated to i \n",
    "            step_normalize(all_predictors())\n",
    "# # Recipe 02:\n",
    "# Age + played_hours\n",
    "rc_AH <- recipe(subscribe ~ Age + played_hours, player_train) |>\n",
    "            step_dummy(all_nominal_predictors()) |>\n",
    "            step_log(played_hours, Age) |>\n",
    "            step_zv(all_predictors()) |> # Used to remove zero-variance variable after wrangling, gender_Other category had no rows associated to i \n",
    "            step_normalize(all_predictors())\n",
    "\n",
    "# Recipe 03:\n",
    "# Age + gender \n",
    "rc_AG <- recipe(subscribe ~ Age + gender, player_train) |>\n",
    "            step_dummy(all_nominal_predictors()) |>\n",
    "            step_log(Age) |>\n",
    "            step_zv(all_predictors()) |> # Used to remove zero-variance variable after wrangling, gender_Other category had no rows associated to i \n",
    "            step_normalize(all_predictors())\n",
    "\n",
    "# Recipe 04:\n",
    "# gender + played_hours\n",
    "rc_GH <- recipe(subscribe ~ gender + played_hours, player_train) |>\n",
    "            step_dummy(all_nominal_predictors()) |>\n",
    "            step_log(played_hours) |>\n",
    "            step_zv(all_predictors()) |> # Used to remove zero-variance variable after wrangling, gender_Other category had no rows associated to i \n",
    "            step_normalize(all_predictors())\n",
    "\n",
    "# Recipe 05:\n",
    "# Age\n",
    "rc_A <- recipe(subscribe ~ Age, player_train) |>\n",
    "            step_dummy(all_nominal_predictors()) |>\n",
    "            step_log(Age) |>\n",
    "            step_zv(all_predictors()) |> # Used to remove zero-variance variable after wrangling, gender_Other category had no rows associated to i \n",
    "            step_normalize(all_predictors())\n",
    "\n",
    "# Recipe 06:\n",
    "# gender\n",
    "rc_G <- recipe(subscribe ~ gender, player_train) |>\n",
    "            step_dummy(all_nominal_predictors()) |>\n",
    "            step_zv(all_predictors()) |> # Used to remove zero-variance variable after wrangling, gender_Other category had no rows associated to i \n",
    "            step_normalize(all_predictors())\n",
    "\n",
    "# Recipe 07:\n",
    "# played_hours\n",
    "rc_H <- recipe(subscribe ~ played_hours, player_train) |>\n",
    "            step_dummy(all_nominal_predictors()) |>\n",
    "            step_log(played_hours) |>\n",
    "            step_zv(all_predictors()) |> # Used to remove zero-variance variable after wrangling, gender_Other category had no rows associated to i \n",
    "            step_normalize(all_predictors())\n",
    "# Recipe 08:\n",
    "# Age + gender + played_hours + experience\n",
    "rc_AGHE <- recipe(subscribe ~ Age + gender + played_hours + experience, data = player_train) |>\n",
    "            step_dummy(all_nominal_predictors()) |>\n",
    "            step_log(played_hours, Age) |>\n",
    "            step_zv(all_predictors()) |> # Used to remove zero-variance variable after wrangling, gender_Other category had no rows associated to i \n",
    "            step_normalize(all_predictors())\n",
    "\n",
    "# Recipe 09:\n",
    "# gender + played_hours + experience\n",
    "rc_GHE <- recipe(subscribe ~ gender + played_hours + experience, data = player_train) |>\n",
    "            step_dummy(all_nominal_predictors()) |>\n",
    "            step_log(played_hours) |>\n",
    "            step_zv(all_predictors()) |> # Used to remove zero-variance variable after wrangling, gender_Other category had no rows associated to i \n",
    "            step_normalize(all_predictors())\n",
    "\n",
    "# Recipe 10:\n",
    "# played_hours + experience\n",
    "rc_HE <- recipe(subscribe ~played_hours + experience, data = player_train) |>\n",
    "            step_dummy(all_nominal_predictors()) |>\n",
    "            step_zv(all_predictors()) |> # Used to remove zero-variance variable after wrangling, gender_Other category had no rows associated to i \n",
    "            step_normalize(all_predictors())\n",
    "\n",
    "# Recipe 11:\n",
    "# gender + played_hours + experience\n",
    "rc_E <- recipe(subscribe ~ experience, data = player_train) |>\n",
    "            step_dummy(all_nominal_predictors()) |>\n",
    "            step_zv(all_predictors()) |> # Used to remove zero-variance variable after wrangling, gender_Other category had no rows associated to i \n",
    "            step_normalize(all_predictors())\n",
    "\n",
    "# Recipe 12:\n",
    "# gender + experience\n",
    "rc_GE <- recipe(subscribe ~ gender + experience, data = player_train) |>\n",
    "            step_dummy(all_nominal_predictors()) |>\n",
    "            step_zv(all_predictors()) |> # Used to remove zero-variance variable after wrangling, gender_Other category had no rows associated to i \n",
    "            step_normalize(all_predictors())\n",
    "\n",
    "# Recipe 13:\n",
    "# Age + played_hours + experience\n",
    "rc_AHE <- recipe(subscribe ~ Age + played_hours + experience, data = player_train) |>\n",
    "            step_dummy(all_nominal_predictors()) |>\n",
    "            step_zv(all_predictors()) |> # Used to remove zero-variance variable after wrangling, gender_Other category had no rows associated to i \n",
    "            step_normalize(all_predictors())\n",
    "\n",
    "# Recipe 14:\n",
    "# Age + experience\n",
    "rc_AE <- recipe(subscribe ~ Age+ experience, data = player_train) |>\n",
    "            step_dummy(all_nominal_predictors()) |>\n",
    "            step_zv(all_predictors()) |> # Used to remove zero-variance variable after wrangling, gender_Other category had no rows associated to i \n",
    "            step_normalize(all_predictors())\n",
    "\n",
    "# Recipe 15:\n",
    "# Age + gender + experience\n",
    "rc_AGE <- recipe(subscribe ~ Age + gender + experience, data = player_train) |>\n",
    "            step_dummy(all_nominal_predictors()) |>\n",
    "            step_zv(all_predictors()) |> # Used to remove zero-variance variable after wrangling, gender_Other category had no rows associated to i \n",
    "            step_normalize(all_predictors())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601fde82-72f4-46a3-9f2e-02a2c3969a6c",
   "metadata": {},
   "source": [
    "**Recipes:**\n",
    "\n",
    "We've put together 15 recipes to explore all the different combinations of predictors that we will train and test our model on to see which combination performs best at predicting `subscribe` outcomes. We will train using our player training dataset. Our recipe must standardize our factor and numerical variables to remove discrepancies in the evaluation weight. We will be using step_dummy to turn our factors/categorical variables (e.g. experience, gender) into dummy numerical variables, then use step_zv\n",
    "\n",
    "> **Combinations of our recipes:**\n",
    ">\n",
    "> 1. Age + gender + played_hours + experience\n",
    "> 2. Age + played_hours\n",
    "> 3. Age + gender\n",
    "> 4. Gender + played_hours\n",
    "> 5. Gender + played_hours + experience\n",
    "> 6. played_hours + experience\n",
    "> 7. Gender + experience\n",
    "> 8. Age + played_hours + experience\n",
    "> 9. Age + experience\n",
    "> 10. Age + gender + experience\n",
    "> 11. Age\n",
    "> 12. gender\n",
    "> 13. played_hours\n",
    "> 14. experience\n",
    "> 15. Age + gender + experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee08e40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spec set up\n",
    "knn_tune <- nearest_neighbor(weight_func = \"rectangular\", neighbors = tune()) |>\n",
    "            set_engine(\"kknn\") |>\n",
    "            set_mode(\"classification\")\n",
    "\n",
    "#TODO FABIO ... search refractor options to reduce code clutter\n",
    "# Check mean and standard error through collect_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cc48c8b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO FABIO confirm that not having set.seed here is fine as long as set.seed above code cell goes first\n",
    "# K-fold cross-validation\n",
    "set.seed(1234) # Don't change\n",
    "kfolds <- vfold_cv(player_train, v = 5, strata = subscribe)\n",
    "k_vals = tibble(neighbors = seq(from = 1, to = 10, by = 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4ef08a2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO FABIO ~ write up spec use and impl of vfold..?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe2a312a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#TODO FABIO write reasonin + graph to show fold outcome\n",
    "#Note the reason of using 10 10-fold is due to the small size data,\n",
    "#hence it will improve the estimate and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a9b7f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mExecution stopped; returning current results\u001b[39m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simplifying multi-model testing: https://www.youtube.com/watch?v=YZqbOATpjM4&t=139s\n",
    "# Workflow Step\n",
    "workflow_set <- workflow_set(\n",
    "    preproc = list(\n",
    "        AGH = rc_AGH,\n",
    "        AH = rc_AH,\n",
    "        AG = rc_AG,\n",
    "        GH = rc_GH,\n",
    "        A = rc_A,\n",
    "        G = rc_G,\n",
    "        H = rc_H,\n",
    "        AGHE = rc_AGHE,\n",
    "        GHE = rc_GHE,\n",
    "        HE = rc_HE,\n",
    "        E = rc_E,\n",
    "        GE = rc_GE,\n",
    "        AHE = rc_AHE,\n",
    "        AE = rc_AE,\n",
    "        AGE = rc_AGE\n",
    "    ),\n",
    "    models = list(knn_tune),\n",
    "    cross = TRUE\n",
    ")\n",
    "\n",
    "# Tune workflow\n",
    "# set.seed(22)\n",
    "\n",
    "knn_tuned_set <- workflow_map(\n",
    "    workflow_set,\n",
    "    \"tune_grid\",\n",
    "    resamples = kfolds,\n",
    "    grid = k_vals,\n",
    "    seed = 22\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "562dc7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO FABIO write workflow step use case/what it functionally is doing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04a0c0d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "→ \u001b[33m\u001b[1mA\u001b[22m\u001b[39m | \u001b[33mwarning\u001b[39m: \u001b[1m\u001b[22mColumn `played_hours` returned NaN, because variance cannot be calculated and\n",
      "               scaling cannot be used. Consider avoiding `Inf` or `-Inf` values and/or setting\n",
      "               `na_rm = TRUE` before normalizing.\n",
      "\n",
      "There were issues with some computations   \u001b[1m\u001b[33mA\u001b[39m\u001b[22m: x1\n",
      "\n",
      "→ \u001b[31m\u001b[1mB\u001b[22m\u001b[39m | \u001b[31merror\u001b[39m:   incorrect number of dimensions\n",
      "\n",
      "There were issues with some computations   \u001b[1m\u001b[33mA\u001b[39m\u001b[22m: x1\n",
      "There were issues with some computations   \u001b[1m\u001b[33mA\u001b[39m\u001b[22m: x5   \u001b[1m\u001b[31mB\u001b[39m\u001b[22m: x5\n",
      "\n",
      "\n",
      "\n",
      "Warning message:\n",
      "“All models failed. Run `show_notes(.Last.tune.result)` for more information.”\n",
      "→ \u001b[33m\u001b[1mA\u001b[22m\u001b[39m | \u001b[33mwarning\u001b[39m: \u001b[1m\u001b[22mColumn `played_hours` returned NaN, because variance cannot be calculated and\n",
      "               scaling cannot be used. Consider avoiding `Inf` or `-Inf` values and/or setting\n",
      "               `na_rm = TRUE` before normalizing.\n",
      "\n",
      "There were issues with some computations   \u001b[1m\u001b[33mA\u001b[39m\u001b[22m: x1\n",
      "\n",
      "→ \u001b[31m\u001b[1mB\u001b[22m\u001b[39m | \u001b[31merror\u001b[39m:   incorrect number of dimensions\n",
      "\n",
      "There were issues with some computations   \u001b[1m\u001b[33mA\u001b[39m\u001b[22m: x1\n",
      "There were issues with some computations   \u001b[1m\u001b[33mA\u001b[39m\u001b[22m: x5   \u001b[1m\u001b[31mB\u001b[39m\u001b[22m: x5\n",
      "\n",
      "\n",
      "\n",
      "Warning message:\n",
      "“All models failed. Run `show_notes(.Last.tune.result)` for more information.”\n",
      "→ \u001b[33m\u001b[1mA\u001b[22m\u001b[39m | \u001b[33mwarning\u001b[39m: \u001b[1m\u001b[22mWhile computing binary `precision()`, no predicted events were detected (i.e.\n",
      "               `true_positive + false_positive = 0`).\n",
      "               Precision is undefined in this case, and `NA` will be returned.\n",
      "               Note that 7 true event(s) actually occurred for the problematic event level,\n",
      "               FALSE\n",
      "\n",
      "There were issues with some computations   \u001b[1m\u001b[33mA\u001b[39m\u001b[22m: x1\n",
      "\n",
      "There were issues with some computations   \u001b[1m\u001b[33mA\u001b[39m\u001b[22m: x3\n",
      "\n",
      "\n",
      "\n",
      "→ \u001b[33m\u001b[1mA\u001b[22m\u001b[39m | \u001b[33mwarning\u001b[39m: \u001b[1m\u001b[22mWhile computing binary `precision()`, no predicted events were detected (i.e.\n",
      "               `true_positive + false_positive = 0`).\n",
      "               Precision is undefined in this case, and `NA` will be returned.\n",
      "               Note that 7 true event(s) actually occurred for the problematic event level,\n",
      "               FALSE\n",
      "\n"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "\u001b[1m\u001b[33mError\u001b[39m in `dplyr::mutate()`:\u001b[22m\n\u001b[1m\u001b[22m\u001b[36mℹ\u001b[39m In argument: `metrics = purrr::map(result, collect_metrics, summarize\n  = summarize)`.\n\u001b[1mCaused by error in `purrr::map()`:\u001b[22m\n\u001b[1m\u001b[22m\u001b[36mℹ\u001b[39m In index: 1.\n\u001b[1mCaused by error in `estimate_tune_results()`:\u001b[22m\n\u001b[33m!\u001b[39m All models failed. Run `show_notes(.Last.tune.result)` for more information.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1m\u001b[33mError\u001b[39m in `dplyr::mutate()`:\u001b[22m\n\u001b[1m\u001b[22m\u001b[36mℹ\u001b[39m In argument: `metrics = purrr::map(result, collect_metrics, summarize\n  = summarize)`.\n\u001b[1mCaused by error in `purrr::map()`:\u001b[22m\n\u001b[1m\u001b[22m\u001b[36mℹ\u001b[39m In index: 1.\n\u001b[1mCaused by error in `estimate_tune_results()`:\u001b[22m\n\u001b[33m!\u001b[39m All models failed. Run `show_notes(.Last.tune.result)` for more information.\nTraceback:\n",
      "1. collect_metrics(all_metrics_collected)",
      "2. collect_metrics.workflow_set(all_metrics_collected)",
      "3. dplyr::mutate(x, metrics = purrr::map(result, collect_metrics, \n .     summarize = summarize), metrics = purrr::map2(metrics, result, \n .     remove_parameters))",
      "4. mutate.data.frame(x, metrics = purrr::map(result, collect_metrics, \n .     summarize = summarize), metrics = purrr::map2(metrics, result, \n .     remove_parameters))",
      "5. mutate_cols(.data, dplyr_quosures(...), by)",
      "6. withCallingHandlers(for (i in seq_along(dots)) {\n .     poke_error_context(dots, i, mask = mask)\n .     context_poke(\"column\", old_current_column)\n .     new_columns <- mutate_col(dots[[i]], data, mask, new_columns)\n . }, error = dplyr_error_handler(dots = dots, mask = mask, bullets = mutate_bullets, \n .     error_call = error_call, error_class = \"dplyr:::mutate_error\"), \n .     warning = dplyr_warning_handler(state = warnings_state, mask = mask, \n .         error_call = error_call))",
      "7. mutate_col(dots[[i]], data, mask, new_columns)",
      "8. mask$eval_all_mutate(quo)",
      "9. eval()",
      "10. purrr::map(result, collect_metrics, summarize = summarize)",
      "11. map_(\"list\", .x, .f, ..., .progress = .progress)",
      "12. with_indexed_errors(i = i, names = names, error_call = .purrr_error_call, \n  .     call_with_cleanup(map_impl, environment(), .type, .progress, \n  .         n, names, i))",
      "13. withCallingHandlers(expr, error = function(cnd) {\n  .     if (i == 0L) {\n  .     }\n  .     else {\n  .         message <- c(i = \"In index: {i}.\")\n  .         if (!is.null(names) && !is.na(names[[i]]) && names[[i]] != \n  .             \"\") {\n  .             name <- names[[i]]\n  .             message <- c(message, i = \"With name: {name}.\")\n  .         }\n  .         else {\n  .             name <- NULL\n  .         }\n  .         cli::cli_abort(message, location = i, name = name, parent = cnd, \n  .             call = error_call, class = \"purrr_error_indexed\")\n  .     }\n  . })",
      "14. call_with_cleanup(map_impl, environment(), .type, .progress, \n  .     n, names, i)",
      "15. .f(.x[[i]], ...)",
      "16. collect_metrics.tune_results(.x[[i]], ...)",
      "17. estimate_tune_results(x)",
      "18. rlang::abort(\"All models failed. Run `show_notes(.Last.tune.result)` for more information.\")",
      "19. signal_abort(cnd, .file)",
      "20. signalCondition(cnd)",
      "21. (function (cnd) \n  . {\n  .     if (i == 0L) {\n  .     }\n  .     else {\n  .         message <- c(i = \"In index: {i}.\")\n  .         if (!is.null(names) && !is.na(names[[i]]) && names[[i]] != \n  .             \"\") {\n  .             name <- names[[i]]\n  .             message <- c(message, i = \"With name: {name}.\")\n  .         }\n  .         else {\n  .             name <- NULL\n  .         }\n  .         cli::cli_abort(message, location = i, name = name, parent = cnd, \n  .             call = error_call, class = \"purrr_error_indexed\")\n  .     }\n  . })(structure(list(message = \"All models failed. Run `show_notes(.Last.tune.result)` for more information.\", \n  .     trace = structure(list(call = list(IRkernel::main(), kernel$run(), \n  .         handle_shell(), executor$execute(msg), tryCatch(evaluate(request$content$code, \n  .             envir = .GlobalEnv, output_handler = oh, stop_on_error = 1L), \n  .             interrupt = function(cond) {\n  .                 log_debug(\"Interrupt during execution\")\n  .                 interrupted <<- TRUE\n  .             }, error = .self$handle_error), tryCatchList(expr, \n  .             classes, parentenv, handlers), tryCatchOne(tryCatchList(expr, \n  .             names[-nh], parentenv, handlers[-nh]), names[nh], \n  .             parentenv, handlers[[nh]]), doTryCatch(return(expr), \n  .             name, parentenv, handler), tryCatchList(expr, names[-nh], \n  .             parentenv, handlers[-nh]), tryCatchOne(expr, names, \n  .             parentenv, handlers[[1L]]), doTryCatch(return(expr), \n  .             name, parentenv, handler), evaluate(request$content$code, \n  .             envir = .GlobalEnv, output_handler = oh, stop_on_error = 1L), \n  .         evaluate_call(expr, parsed$src[[i]], envir = envir, enclos = enclos, \n  .             debug = debug, last = i == length(out), use_try = stop_on_error != \n  .                 2L, keep_warning = keep_warning, keep_message = keep_message, \n  .             log_echo = log_echo, log_warning = log_warning, output_handler = output_handler, \n  .             include_timing = include_timing), timing_fn(handle(ev <- withCallingHandlers(withVisible(eval_with_user_handlers(expr, \n  .             envir, enclos, user_handlers)), warning = wHandler, \n  .             error = eHandler, message = mHandler))), handle(ev <- withCallingHandlers(withVisible(eval_with_user_handlers(expr, \n  .             envir, enclos, user_handlers)), warning = wHandler, \n  .             error = eHandler, message = mHandler)), try(f, silent = TRUE), \n  .         tryCatch(expr, error = function(e) {\n  .             call <- conditionCall(e)\n  .             if (!is.null(call)) {\n  .                 if (identical(call[[1L]], quote(doTryCatch))) \n  .                   call <- sys.call(-4L)\n  .                 dcall <- deparse(call, nlines = 1L)\n  .                 prefix <- paste(\"Error in\", dcall, \": \")\n  .                 LONG <- 75L\n  .                 sm <- strsplit(conditionMessage(e), \"\\n\")[[1L]]\n  .                 w <- 14L + nchar(dcall, type = \"w\") + nchar(sm[1L], \n  .                   type = \"w\")\n  .                 if (is.na(w)) \n  .                   w <- 14L + nchar(dcall, type = \"b\") + nchar(sm[1L], \n  .                     type = \"b\")\n  .                 if (w > LONG) \n  .                   prefix <- paste0(prefix, \"\\n  \")\n  .             }\n  .             else prefix <- \"Error : \"\n  .             msg <- paste0(prefix, conditionMessage(e), \"\\n\")\n  .             .Internal(seterrmessage(msg[1L]))\n  .             if (!silent && isTRUE(getOption(\"show.error.messages\"))) {\n  .                 cat(msg, file = outFile)\n  .                 .Internal(printDeferredWarnings())\n  .             }\n  .             invisible(structure(msg, class = \"try-error\", condition = e))\n  .         }), tryCatchList(expr, classes, parentenv, handlers), \n  .         tryCatchOne(expr, names, parentenv, handlers[[1L]]), \n  .         doTryCatch(return(expr), name, parentenv, handler), withCallingHandlers(withVisible(eval_with_user_handlers(expr, \n  .             envir, enclos, user_handlers)), warning = wHandler, \n  .             error = eHandler, message = mHandler), withVisible(eval_with_user_handlers(expr, \n  .             envir, enclos, user_handlers)), eval_with_user_handlers(expr, \n  .             envir, enclos, user_handlers), eval(expr, envir, \n  .             enclos), eval(expr, envir, enclos), collect_metrics(all_metrics_collected), \n  .         collect_metrics.workflow_set(all_metrics_collected), \n  .         dplyr::mutate(x, metrics = purrr::map(result, collect_metrics, \n  .             summarize = summarize), metrics = purrr::map2(metrics, \n  .             result, remove_parameters)), mutate.data.frame(x, \n  .             metrics = purrr::map(result, collect_metrics, summarize = summarize), \n  .             metrics = purrr::map2(metrics, result, remove_parameters)), \n  .         mutate_cols(.data, dplyr_quosures(...), by), withCallingHandlers(for (i in seq_along(dots)) {\n  .             poke_error_context(dots, i, mask = mask)\n  .             context_poke(\"column\", old_current_column)\n  .             new_columns <- mutate_col(dots[[i]], data, mask, \n  .                 new_columns)\n  .         }, error = dplyr_error_handler(dots = dots, mask = mask, \n  .             bullets = mutate_bullets, error_call = error_call, \n  .             error_class = \"dplyr:::mutate_error\"), warning = dplyr_warning_handler(state = warnings_state, \n  .             mask = mask, error_call = error_call)), mutate_col(dots[[i]], \n  .             data, mask, new_columns), mask$eval_all_mutate(quo), \n  .         eval(), purrr::map(result, collect_metrics, summarize = summarize), \n  .         map_(\"list\", .x, .f, ..., .progress = .progress), with_indexed_errors(i = i, \n  .             names = names, error_call = .purrr_error_call, call_with_cleanup(map_impl, \n  .                 environment(), .type, .progress, n, names, i)), \n  .         withCallingHandlers(expr, error = function(cnd) {\n  .             if (i == 0L) {\n  .             }\n  .             else {\n  .                 message <- c(i = \"In index: {i}.\")\n  .                 if (!is.null(names) && !is.na(names[[i]]) && \n  .                   names[[i]] != \"\") {\n  .                   name <- names[[i]]\n  .                   message <- c(message, i = \"With name: {name}.\")\n  .                 }\n  .                 else {\n  .                   name <- NULL\n  .                 }\n  .                 cli::cli_abort(message, location = i, name = name, \n  .                   parent = cnd, call = error_call, class = \"purrr_error_indexed\")\n  .             }\n  .         }), call_with_cleanup(map_impl, environment(), .type, \n  .             .progress, n, names, i), .f(.x[[i]], ...), collect_metrics.tune_results(.x[[i]], \n  .             ...), estimate_tune_results(x), rlang::abort(\"All models failed. Run `show_notes(.Last.tune.result)` for more information.\")), \n  .         parent = c(0L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 6L, 9L, 10L, \n  .         4L, 12L, 13L, 13L, 15L, 16L, 17L, 18L, 19L, 13L, 13L, \n  .         13L, 23L, 24L, 0L, 0L, 27L, 27L, 29L, 30L, 30L, 32L, \n  .         33L, 0L, 35L, 36L, 37L, 36L, 36L, 36L, 41L, 42L), visible = c(TRUE, \n  .         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, \n  .         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, \n  .         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, \n  .         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, \n  .         TRUE, TRUE, TRUE, TRUE, TRUE, FALSE), namespace = c(\"IRkernel\", \n  .         NA, \"IRkernel\", NA, \"base\", \"base\", \"base\", \"base\", \"base\", \n  .         \"base\", \"base\", \"evaluate\", \"evaluate\", \"evaluate\", \"evaluate\", \n  .         \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \n  .         \"evaluate\", \"base\", \"base\", \"tune\", \"workflowsets\", \"dplyr\", \n  .         \"dplyr\", \"dplyr\", \"base\", \"dplyr\", NA, \"dplyr\", \"purrr\", \n  .         \"purrr\", \"purrr\", \"base\", \"purrr\", \"tune\", \"tune\", \"tune\", \n  .         \"rlang\"), scope = c(\"::\", NA, \"local\", NA, \"::\", \"local\", \n  .         \"local\", \"local\", \"local\", \"local\", \"local\", \"::\", \":::\", \n  .         \"local\", \"local\", \"::\", \"::\", \"local\", \"local\", \"local\", \n  .         \"::\", \"::\", \":::\", \"::\", \"::\", \"::\", \":::\", \"::\", \":::\", \n  .         \":::\", \"::\", \":::\", NA, \"local\", \"::\", \":::\", \":::\", \n  .         \"::\", \":::\", \"local\", \":::\", \"::\", \"::\"), error_frame = c(FALSE, \n  .         FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, \n  .         FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, \n  .         FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, \n  .         FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, \n  .         FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, \n  .         TRUE, FALSE)), row.names = c(NA, -43L), version = 2L, class = c(\"rlang_trace\", \n  .     \"rlib_trace\", \"tbl\", \"data.frame\")), parent = NULL, rlang = list(\n  .         inherit = TRUE), call = estimate_tune_results(x)), class = c(\"rlang_error\", \n  . \"error\", \"condition\")))",
      "22. cli::cli_abort(message, location = i, name = name, parent = cnd, \n  .     call = error_call, class = \"purrr_error_indexed\")",
      "23. rlang::abort(message, ..., call = call, use_cli_format = TRUE, \n  .     .frame = .frame)",
      "24. signal_abort(cnd, .file)",
      "25. signalCondition(cnd)",
      "26. (function (cnd) \n  . {\n  .     local_error_context(dots, i = frame[[i_sym]], mask = mask)\n  .     if (inherits(cnd, \"dplyr:::internal_error\")) {\n  .         parent <- error_cnd(message = bullets(cnd))\n  .     }\n  .     else {\n  .         parent <- cnd\n  .     }\n  .     message <- c(cnd_bullet_header(action), i = if (has_active_group_context(mask)) cnd_bullet_cur_group_label())\n  .     abort(message, class = error_class, parent = parent, call = error_call)\n  . })(structure(list(message = c(i = \"In index: 1.\"), parent = structure(list(\n  .     message = \"All models failed. Run `show_notes(.Last.tune.result)` for more information.\", \n  .     trace = structure(list(call = list(IRkernel::main(), kernel$run(), \n  .         handle_shell(), executor$execute(msg), tryCatch(evaluate(request$content$code, \n  .             envir = .GlobalEnv, output_handler = oh, stop_on_error = 1L), \n  .             interrupt = function(cond) {\n  .                 log_debug(\"Interrupt during execution\")\n  .                 interrupted <<- TRUE\n  .             }, error = .self$handle_error), tryCatchList(expr, \n  .             classes, parentenv, handlers), tryCatchOne(tryCatchList(expr, \n  .             names[-nh], parentenv, handlers[-nh]), names[nh], \n  .             parentenv, handlers[[nh]]), doTryCatch(return(expr), \n  .             name, parentenv, handler), tryCatchList(expr, names[-nh], \n  .             parentenv, handlers[-nh]), tryCatchOne(expr, names, \n  .             parentenv, handlers[[1L]]), doTryCatch(return(expr), \n  .             name, parentenv, handler), evaluate(request$content$code, \n  .             envir = .GlobalEnv, output_handler = oh, stop_on_error = 1L), \n  .         evaluate_call(expr, parsed$src[[i]], envir = envir, enclos = enclos, \n  .             debug = debug, last = i == length(out), use_try = stop_on_error != \n  .                 2L, keep_warning = keep_warning, keep_message = keep_message, \n  .             log_echo = log_echo, log_warning = log_warning, output_handler = output_handler, \n  .             include_timing = include_timing), timing_fn(handle(ev <- withCallingHandlers(withVisible(eval_with_user_handlers(expr, \n  .             envir, enclos, user_handlers)), warning = wHandler, \n  .             error = eHandler, message = mHandler))), handle(ev <- withCallingHandlers(withVisible(eval_with_user_handlers(expr, \n  .             envir, enclos, user_handlers)), warning = wHandler, \n  .             error = eHandler, message = mHandler)), try(f, silent = TRUE), \n  .         tryCatch(expr, error = function(e) {\n  .             call <- conditionCall(e)\n  .             if (!is.null(call)) {\n  .                 if (identical(call[[1L]], quote(doTryCatch))) \n  .                   call <- sys.call(-4L)\n  .                 dcall <- deparse(call, nlines = 1L)\n  .                 prefix <- paste(\"Error in\", dcall, \": \")\n  .                 LONG <- 75L\n  .                 sm <- strsplit(conditionMessage(e), \"\\n\")[[1L]]\n  .                 w <- 14L + nchar(dcall, type = \"w\") + nchar(sm[1L], \n  .                   type = \"w\")\n  .                 if (is.na(w)) \n  .                   w <- 14L + nchar(dcall, type = \"b\") + nchar(sm[1L], \n  .                     type = \"b\")\n  .                 if (w > LONG) \n  .                   prefix <- paste0(prefix, \"\\n  \")\n  .             }\n  .             else prefix <- \"Error : \"\n  .             msg <- paste0(prefix, conditionMessage(e), \"\\n\")\n  .             .Internal(seterrmessage(msg[1L]))\n  .             if (!silent && isTRUE(getOption(\"show.error.messages\"))) {\n  .                 cat(msg, file = outFile)\n  .                 .Internal(printDeferredWarnings())\n  .             }\n  .             invisible(structure(msg, class = \"try-error\", condition = e))\n  .         }), tryCatchList(expr, classes, parentenv, handlers), \n  .         tryCatchOne(expr, names, parentenv, handlers[[1L]]), \n  .         doTryCatch(return(expr), name, parentenv, handler), withCallingHandlers(withVisible(eval_with_user_handlers(expr, \n  .             envir, enclos, user_handlers)), warning = wHandler, \n  .             error = eHandler, message = mHandler), withVisible(eval_with_user_handlers(expr, \n  .             envir, enclos, user_handlers)), eval_with_user_handlers(expr, \n  .             envir, enclos, user_handlers), eval(expr, envir, \n  .             enclos), eval(expr, envir, enclos), collect_metrics(all_metrics_collected), \n  .         collect_metrics.workflow_set(all_metrics_collected), \n  .         dplyr::mutate(x, metrics = purrr::map(result, collect_metrics, \n  .             summarize = summarize), metrics = purrr::map2(metrics, \n  .             result, remove_parameters)), mutate.data.frame(x, \n  .             metrics = purrr::map(result, collect_metrics, summarize = summarize), \n  .             metrics = purrr::map2(metrics, result, remove_parameters)), \n  .         mutate_cols(.data, dplyr_quosures(...), by), withCallingHandlers(for (i in seq_along(dots)) {\n  .             poke_error_context(dots, i, mask = mask)\n  .             context_poke(\"column\", old_current_column)\n  .             new_columns <- mutate_col(dots[[i]], data, mask, \n  .                 new_columns)\n  .         }, error = dplyr_error_handler(dots = dots, mask = mask, \n  .             bullets = mutate_bullets, error_call = error_call, \n  .             error_class = \"dplyr:::mutate_error\"), warning = dplyr_warning_handler(state = warnings_state, \n  .             mask = mask, error_call = error_call)), mutate_col(dots[[i]], \n  .             data, mask, new_columns), mask$eval_all_mutate(quo), \n  .         eval(), purrr::map(result, collect_metrics, summarize = summarize), \n  .         map_(\"list\", .x, .f, ..., .progress = .progress), with_indexed_errors(i = i, \n  .             names = names, error_call = .purrr_error_call, call_with_cleanup(map_impl, \n  .                 environment(), .type, .progress, n, names, i)), \n  .         withCallingHandlers(expr, error = function(cnd) {\n  .             if (i == 0L) {\n  .             }\n  .             else {\n  .                 message <- c(i = \"In index: {i}.\")\n  .                 if (!is.null(names) && !is.na(names[[i]]) && \n  .                   names[[i]] != \"\") {\n  .                   name <- names[[i]]\n  .                   message <- c(message, i = \"With name: {name}.\")\n  .                 }\n  .                 else {\n  .                   name <- NULL\n  .                 }\n  .                 cli::cli_abort(message, location = i, name = name, \n  .                   parent = cnd, call = error_call, class = \"purrr_error_indexed\")\n  .             }\n  .         }), call_with_cleanup(map_impl, environment(), .type, \n  .             .progress, n, names, i), .f(.x[[i]], ...), collect_metrics.tune_results(.x[[i]], \n  .             ...), estimate_tune_results(x), rlang::abort(\"All models failed. Run `show_notes(.Last.tune.result)` for more information.\")), \n  .         parent = c(0L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 6L, 9L, 10L, \n  .         4L, 12L, 13L, 13L, 15L, 16L, 17L, 18L, 19L, 13L, 13L, \n  .         13L, 23L, 24L, 0L, 0L, 27L, 27L, 29L, 30L, 30L, 32L, \n  .         33L, 0L, 35L, 36L, 37L, 36L, 36L, 36L, 41L, 42L), visible = c(TRUE, \n  .         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, \n  .         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, \n  .         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, \n  .         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, \n  .         TRUE, TRUE, TRUE, TRUE, TRUE, FALSE), namespace = c(\"IRkernel\", \n  .         NA, \"IRkernel\", NA, \"base\", \"base\", \"base\", \"base\", \"base\", \n  .         \"base\", \"base\", \"evaluate\", \"evaluate\", \"evaluate\", \"evaluate\", \n  .         \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \n  .         \"evaluate\", \"base\", \"base\", \"tune\", \"workflowsets\", \"dplyr\", \n  .         \"dplyr\", \"dplyr\", \"base\", \"dplyr\", NA, \"dplyr\", \"purrr\", \n  .         \"purrr\", \"purrr\", \"base\", \"purrr\", \"tune\", \"tune\", \"tune\", \n  .         \"rlang\"), scope = c(\"::\", NA, \"local\", NA, \"::\", \"local\", \n  .         \"local\", \"local\", \"local\", \"local\", \"local\", \"::\", \":::\", \n  .         \"local\", \"local\", \"::\", \"::\", \"local\", \"local\", \"local\", \n  .         \"::\", \"::\", \":::\", \"::\", \"::\", \"::\", \":::\", \"::\", \":::\", \n  .         \":::\", \"::\", \":::\", NA, \"local\", \"::\", \":::\", \":::\", \n  .         \"::\", \":::\", \"local\", \":::\", \"::\", \"::\"), error_frame = c(FALSE, \n  .         FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, \n  .         FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, \n  .         FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, \n  .         FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, \n  .         FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, \n  .         TRUE, FALSE)), row.names = c(NA, -43L), version = 2L, class = c(\"rlang_trace\", \n  .     \"rlib_trace\", \"tbl\", \"data.frame\")), parent = NULL, rlang = list(\n  .         inherit = TRUE), call = estimate_tune_results(x)), class = c(\"rlang_error\", \n  . \"error\", \"condition\")), location = 1L, name = NULL, rlang = list(\n  .     inherit = TRUE), call = purrr::map(result, collect_metrics, \n  .     summarize = summarize), use_cli_format = TRUE), class = c(\"purrr_error_indexed\", \n  . \"rlang_error\", \"error\", \"condition\")))",
      "27. abort(message, class = error_class, parent = parent, call = error_call)",
      "28. signal_abort(cnd, .file)"
     ]
    }
   ],
   "source": [
    "\n",
    "options(repr.plot.width = 15, repr.plot.height = 5)\n",
    "# Get metrics all in one\n",
    "all_metrics_collected <- workflow_map(\n",
    "    knn_tuned_set,\n",
    "    metrics = metric_set(accuracy, recall, precision)\n",
    ") \n",
    "\n",
    "all_metrics_collected_res <- collect_metrics(all_metrics_collected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6706816-cea5-4d6b-b026-57f2a1efc858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_All_met_tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c22243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_metrics_collected_res\n",
    "#TODO TMP trying to regain neighbours\n",
    "res_All_met_tuned <- all_metrics_collected |>\n",
    "            rowwise() |>\n",
    "            mutate(metrics = list(collect_metrics(result)),\n",
    "                  wflow_id = fct_recode(wflow_id,\n",
    "                      \"Age, gender and played hours\" = \"AGH_nearest_neighbor\",\n",
    "                      \"Gender and played hours\" = \"GH_nearest_neighbor\",\n",
    "                      \"Gender\" = \"G_nearest_neighbor\",\n",
    "                      \"Age and gender\" = \"AG_nearest_neighbor\",\n",
    "                      \"Age and played hours\" = \"AH_nearest_neighbor\",\n",
    "                      \"Age\" = \"A_nearest_neighbor\",\n",
    "                      \"Played hours\" = \"H_nearest_neighbor\",\n",
    "                    \"Age, Gender, played_hours and experience\" = \"AGHE_nearest_neighbor\",\n",
    "                   \"Gender, played_hours and experience\" = \"GHE_nearest_neighbor\",   \n",
    "                    \"played_hours and experience\" = \"HE_nearest_neighbor\", \n",
    "                     \"experience\" = \"E_nearest_neighbor\", \n",
    "                     \"Gender and experience\" = \"GE_nearest_neighbor\",\n",
    "                     \"Age, played_hours and experience\" = \"AHE_nearest_neighbor\",\n",
    "                     \"Age and experience\" = \"AGE_nearest_neighbor\",\n",
    "                      \"Age, Gender and experience\" = \"AGE_nearest_neighbor\",\n",
    "                  )) |>\n",
    "            unnest(metrics) |>\n",
    "            select(wflow_id, .metric, mean, neighbors, std_err) |>\n",
    "            arrange(desc(mean))\n",
    "# ADD RANKING\n",
    "ranked_met <- res_All_met_tuned |>\n",
    "            mutate(rank = seq(1, nrow(res_All_met_tuned), 1))\n",
    "# ranked_met\n",
    "# Visualize result\n",
    "options(repr.plot.width = 8, repr.plot.height = 5)\n",
    "\n",
    "plot_met_tmp <- ggplot(ranked_met, aes(x = rank, y = mean, color = wflow_id)) +\n",
    "                geom_point() +\n",
    "                 # geom_errorbar(aes(ymin = mean - std_err, ymax = mean + std_err), width=.2, #TODO TMP test to see if use line but std_err may not be correct\n",
    "                 # position = position_dodge(0.05)) + \n",
    "                ylim(c(0,1)) + \n",
    "                labs(x = \"Ranke by metric mean\", y = \"Metric mean percentage\", color = \"Predictor Combinations\") \n",
    "\n",
    "# plot_met_tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c549ae",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# METRICS set up Graphs\n",
    "# Accuracy\n",
    "acc_met <- res_All_met_tuned |>\n",
    "            filter(.metric ==  \"accuracy\")\n",
    "acc_met_rank <- mutate(acc_met, rank = seq(1, nrow(acc_met), 1)) |>\n",
    "                slice(1:50)\n",
    "acc_met_plot <- plot_met_tmp %+% acc_met_rank +\n",
    "                guides(color = \"none\") + \n",
    "                ggtitle(\"Ranked by Accuracy Mean\") \n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "# Precision\n",
    "prec_met <- res_All_met_tuned |>\n",
    "            filter(.metric ==  \"precision\")\n",
    "prec_met_rank <- mutate(prec_met, rank = seq(1, nrow(prec_met), 1)) |>\n",
    "                slice(1:50)\n",
    "prec_met_plot <- plot_met_tmp %+% prec_met_rank +\n",
    "                guides(color = \"none\") + \n",
    "                ggtitle(\"Ranked by Precision Mean\") \n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "# Recall\n",
    "rec_met <- res_All_met_tuned |>\n",
    "            filter(.metric ==  \"recall\")\n",
    "rec_met_rank <- mutate(rec_met, rank = seq(1, nrow(rec_met), 1)) |>\n",
    "               slice(1:50)\n",
    "rec_met_plot <- plot_met_tmp %+% rec_met_rank +\n",
    "                ggtitle(\"Ranked by Recall Mean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c91f5b7-9c72-402d-949a-e8c20624c856",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#TODO conf amat graph of each category ???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b63dd6-7d72-4e09-9301-0fad53126589",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# All results side by side\n",
    "options(repr.plot.width = 15, repr.plot.height = 5)\n",
    "#TODO fabio add std_err line but also combine color and shape legend\n",
    "# side_by_side_met\n",
    "acc_met_plot + prec_met_plot + rec_met_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff94b9c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width = 8, repr.plot.height = 5)\n",
    "# Accuracy tuned metric plot\n",
    "acc_met_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730fcc25",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width = 8, repr.plot.height = 5)\n",
    "# Accuracy tuned metric prec\n",
    "prec_met_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3784b1-ab2a-4f66-ac53-e7c1f7ccf327",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#TODO write about the result prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da4121d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width = 8, repr.plot.height = 5)\n",
    "# Accuracy tuned metric rec\n",
    "rec_met_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58df404-2708-45f4-a38e-adab6a992916",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#TODO write about the result rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb486b7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width = 8, repr.plot.height = 5)\n",
    "# LINE Graph showing k performance for each recipe\n",
    "overlapped_line_acc <- ggplot(acc_met, aes(x = neighbors, y = mean, color = wflow_id)) +\n",
    "                        geom_line() +\n",
    "                        geom_point() + \n",
    "                        ylim(c(0, 1)) + \n",
    "                        scale_x_continuous(breaks = seq(0,5,1)) +\n",
    "                        labs(\n",
    "                            x = \"K neighbors\", \n",
    "                            y = \"Accuracy percentage\",  \n",
    "                            color = \"Predictor Combinations\",\n",
    "                        )  +\n",
    "                        ggtitle(\"K-nn performance of K by Accuracy\")\n",
    "                        \n",
    "\n",
    "overlapped_line_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dd7586-71de-4af5-80b7-2db7df5c50eb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#TODO FABIO discuss on concerns for high k doing so well, should concider reducing k for some options maybe?\n",
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940c754b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width = 8, repr.plot.height = 5)\n",
    "# LINE Graph showing k performance for each recipe Using precision\n",
    "overlapped_line_prec <- overlapped_line_acc %+% prec_met +\n",
    "                        ggtitle(\"K-nn performance of K by Precision\")\n",
    "overlapped_line_prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e1754b-c2ea-4eac-b7e8-0a28f916c56a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#TODO FABIO discuss on concerns for high k doing so well, should concider reducing k for some options maybe?\n",
    "# Precission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50aa37c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width = 8, repr.plot.height = 5)\n",
    "# LINE Graph showing k performance for each recipe Using precision\n",
    "overlapped_line_rec <- overlapped_line_acc %+% rec_met +\n",
    "                        ggtitle(\"K-nn performance of K by Recall\")\n",
    "overlapped_line_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a32c0c9-bd44-4d9a-9c51-6bd0c08a3a22",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#TODO FABIO discuss on concerns for high k doing so well, should concider reducing k for some options maybe?\n",
    "# Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d1a47e-153f-44e1-a518-a2745d73866b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# SELECT FINAL MODEL AND TRAIN \n",
    "set.seed(22)\n",
    "#TMP DELETE\n",
    "# player_test\n",
    "# player_train\n",
    "\n",
    "# Model Age + played_hours with k = 5 recipe(rc_AH)\n",
    "final_md_spec <- nearest_neighbor(weight_func = \"rectangular\", neighbors = 5) |>\n",
    "            set_engine(\"kknn\") |>\n",
    "            set_mode(\"classification\")\n",
    "\n",
    "final_md_fit <- workflow() |>\n",
    "            add_recipe(rc_AE) |>\n",
    "            add_model(final_md_spec) |>\n",
    "            fit(player_train)\n",
    "# final_md_fit\n",
    "\n",
    "\n",
    "########\n",
    "final_md_pred <- predict(final_md_fit, player_test) |>\n",
    "                bind_cols(player_test)\n",
    "# final_md_pred\n",
    "final_md_met <- final_md_pred |>\n",
    "                metrics(truth = subscribe, estimate = .pred_class)\n",
    "final_md_met\n",
    "final_md_mat <- conf_mat(final_md_pred, truth = subscribe, estimate = .pred_class)\n",
    "final_md_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62786844-6c81-4cf6-a12d-f64d4b020fd2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ef9d11",
   "metadata": {},
   "source": [
    "### Player Type Exploration: \"TODO may need renaming\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbab2c8",
   "metadata": {},
   "source": [
    "## Discussion:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c0e9d1",
   "metadata": {},
   "source": [
    "### Reference \n",
    "\n",
    "##### Data Science programming techniques and approaches:\n",
    "- **workflowset:** https://workflowsets.tidymodels.org/\n",
    "- **Clustering prediction:** https://www.youtube.com/watch?v=z57i2GVcdww\n",
    "- **Simplifying multi-model set up + testing:** https://www.youtube.com/watch?v=YZqbOATpjM4\n",
    "- **Tuning and comparing models using Workflowse:** https://workflowsets.tidymodels.org/articles/tuning-and-comparing-models.html\n",
    "- **Useful example of report/tutorial for tidy models:** https://optimumsportsperformance.com/blog/k-nearest-neighbor-tidymodels-tutorial/\n",
    "- **Additional breakdown on workflow_set usage:** https://www.youtube.com/watch?v=R95lWUDtL5A\n",
    "- **Workflow_set documentation on results:** https://workflowsets.tidymodels.org/reference/collect_metrics.workflow_set\n",
    "- **Workflow_set tunning and comparing documentation:** https://workflowsets.tidymodels.org/articles/tuning-and-comparing-models\n",
    "- **Plot ggpair examples breakdown for reference:** https://r-charts.com/correlation/ggpairs/\n",
    "- **margin of error plot template models:** https://www.sthda.com/english/wiki/ggplot2-error-bars-quick-start-guide-r-software-and-data-visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67a344f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#TODO tmp info grounds\n",
    "\n",
    "?guides"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
